# =============================================================================
# TÃœRK AKADEMÄ°K TEZ ARAÅTIRMA ASÄ°STANI
# =============================================================================
# Bu uygulama, RAG (Retrieval Augmented Generation) mimarisi kullanarak
# TÃ¼rk akademik tezlerinden bilgi Ã§Ä±karÄ±r ve kullanÄ±cÄ± sorularÄ±na yanÄ±t verir.
#
# KullanÄ±lan Teknolojiler:
# - LangChain: RAG pipeline yÃ¶netimi
# - ChromaDB: VektÃ¶r veritabanÄ±
# - Groq API (Llama 3.1): Dil modeli
# - Streamlit: Web arayÃ¼zÃ¼
# - Hugging Face Datasets: Veri kaynaÄŸÄ±
# =============================================================================

import streamlit as st  # Web arayÃ¼zÃ¼ iÃ§in Streamlit kÃ¼tÃ¼phanesi
import os  # Ä°ÅŸletim sistemi iÅŸlemleri (environment variables, dizin oluÅŸturma)
from langchain_chroma import Chroma  # ChromaDB vektÃ¶r veritabanÄ± entegrasyonu
from langchain_groq import ChatGroq  # Groq API ile Llama model entegrasyonu
from langchain.chains import create_retrieval_chain  # RAG zinciri oluÅŸturma
from langchain.chains.combine_documents import create_stuff_documents_chain  # Belgeleri birleÅŸtirme
from langchain.prompts import ChatPromptTemplate  # Prompt ÅŸablonu oluÅŸturma
from langchain.schema import Document  # LangChain Document formatÄ±
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Metin bÃ¶lme
from langchain.embeddings.base import Embeddings  # Embedding base sÄ±nÄ±fÄ±
import hashlib  # Hash fonksiyonlarÄ± (custom embedding iÃ§in)
from datasets import load_dataset  # Hugging Face datasets kÃ¼tÃ¼phanesi
from dotenv import load_dotenv  # âœ… .env dosyasÄ±nÄ± okumak iÃ§in eklendi
load_dotenv()  # .env iÃ§indeki deÄŸiÅŸkenleri yÃ¼kle

# =============================================================================
# HUGGING FACE CACHE AYARLARI
# =============================================================================
# Hugging Face Spaces'te varsayÄ±lan cache dizini (~/.cache) yazma korumalÄ±dÄ±r.
# Bu yÃ¼zden tÃ¼m cache iÅŸlemlerini /tmp dizinine yÃ¶nlendiriyoruz.
# /tmp dizini geÃ§ici ve yazÄ±labilir bir dizindir.

os.environ["HF_HOME"] = "/tmp/huggingface"  # Ana Hugging Face dizini
os.environ["HF_DATASETS_CACHE"] = "/tmp/huggingface/datasets"  # Dataset cache
os.environ["TRANSFORMERS_CACHE"] = "/tmp/huggingface/transformers"  # Model cache
os.environ["XDG_CACHE_HOME"] = "/tmp/huggingface"  # Genel cache dizini
os.environ["HF_HUB_CACHE"] = "/tmp/huggingface/hub"  # Hub cache

# Cache dizinlerini oluÅŸtur (eÄŸer yoksa)
# exist_ok=True: Dizin zaten varsa hata verme
os.makedirs("/tmp/huggingface/datasets", exist_ok=True)
os.makedirs("/tmp/huggingface/transformers", exist_ok=True)
os.makedirs("/tmp/huggingface/hub", exist_ok=True)

# =============================================================================
# STREAMLIT SAYFA AYARLARI
# =============================================================================
# Uygulama baÅŸlÄ±ÄŸÄ±, ikonu ve layout ayarlarÄ±
st.set_page_config(
    page_title="TÃ¼rk Akademik Tez AsistanÄ±",  # Browser sekmesinde gÃ¶rÃ¼nen baÅŸlÄ±k
    page_icon="ğŸ“",  # Browser sekmesinde gÃ¶rÃ¼nen ikon
    layout="wide"  # GeniÅŸ layout (varsayÄ±lan: centered)
)

# =============================================================================
# CUSTOM EMBEDDING SINIFI
# =============================================================================
# Model indirmeyi Ã¶nlemek iÃ§in hash tabanlÄ± basit bir embedding sÄ±nÄ±fÄ±.
# Her metin SHA-256 hash'i ile vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
# Dezavantaj: Semantik benzerlik yakalamaz (sadece Ã¶zdeÅŸlik)
# Avantaj: HÄ±zlÄ±, model indirmez, offline Ã§alÄ±ÅŸÄ±r

class SimpleHashEmbeddings(Embeddings):
    """
    Hash tabanlÄ± embedding sÄ±nÄ±fÄ±.
    
    Metinleri SHA-256 hash algoritmasÄ± ile sabit boyutlu vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    Model indirme gerektirmez, bu yÃ¼zden deployment sorunlarÄ± yaÅŸanmaz.
    
    Args:
        dimension (int): Embedding vektÃ¶rÃ¼nÃ¼n boyutu (varsayÄ±lan: 384)
    """
    
    def __init__(self, dimension=384):
        """
        SÄ±nÄ±fÄ± baÅŸlat.
        
        Args:
            dimension (int): Ã‡Ä±ktÄ± vektÃ¶rÃ¼nÃ¼n boyutu
        """
        self.dimension = dimension  # VektÃ¶r boyutunu sakla
    
    def embed_documents(self, texts):
        """
        Birden fazla metni embedding vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼r.
        
        Args:
            texts (List[str]): Embedding'e Ã§evrilecek metin listesi
            
        Returns:
            List[List[float]]: Her metin iÃ§in embedding vektÃ¶rÃ¼
        """
        embeddings = []  # SonuÃ§ vektÃ¶rlerini sakla
        
        for text in texts:
            # 1. Metni SHA-256 ile hash'le
            hash_obj = hashlib.sha256(text.encode())  # UTF-8 encode + hash
            
            # 2. Hash'i byte dizisine Ã§evir
            hash_bytes = hash_obj.digest()  # 32 byte (256 bit)
            
            # 3. Byte'larÄ± 0-1 arasÄ± float'a normalize et
            # Ä°lk 'dimension' kadar byte'Ä± kullan
            vector = [float(b) / 255.0 for b in hash_bytes[:self.dimension]]
            
            # 4. EÄŸer dimension > 32 ise, geri kalanÄ±nÄ± 0 ile doldur
            vector += [0.0] * (self.dimension - len(vector))
            
            embeddings.append(vector)  # Listeye ekle
        
        return embeddings
    
    def embed_query(self, text):
        """
        Tek bir sorgu metnini embedding vektÃ¶rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼r.
        
        Args:
            text (str): Embedding'e Ã§evrilecek metin
            
        Returns:
            List[float]: Embedding vektÃ¶rÃ¼
        """
        # embed_documents fonksiyonunu kullan (tek elemanlÄ± liste)
        return self.embed_documents([text])[0]

# =============================================================================
# RAG SÄ°STEMÄ° KURULUM FONKSÄ°YONU
# =============================================================================
# Bu fonksiyon tÃ¼m RAG pipeline'Ä±nÄ± oluÅŸturur:
# 1. Dataset yÃ¼kleme
# 2. Metin bÃ¶lme (chunking)
# 3. Embedding + VektÃ¶r DB oluÅŸturma
# 4. Retriever + LLM + Prompt birleÅŸtirme

@st.cache_resource  # Sonucu cache'le, her reload'da yeniden Ã§alÄ±ÅŸtÄ±rma
def setup_rag_system(api_key, hf_token=None):
    """
    RAG sistemini kur ve hazÄ±rla.
    
    Args:
        api_key (str): Groq API key
        hf_token (str, optional): Hugging Face token (private dataset iÃ§in)
        
    Returns:
        Pipeline: KullanÄ±ma hazÄ±r RAG chain veya None (hata durumunda)
    """
    
    # --- API Key KontrolÃ¼ ---
    if not api_key:
        st.error("âŒ GROQ_API_KEY bulunamadÄ±.")
        return None
    
    st.info("ğŸš€ RAG sistemi baÅŸlatÄ±lÄ±yor...")

    # --- 1. EMBEDDING MODELÄ° OLUÅTUR ---
    # Hash tabanlÄ± custom embedding (256 boyutlu vektÃ¶r)
    embedding_model = SimpleHashEmbeddings(dimension=256)
    
    # --- 2. DÄ°L MODELÄ°NÄ° OLUÅTUR ---
    # Groq API ile Llama 3.1 modeli (8 milyar parametre, instant versiyonu)
    llm = ChatGroq(
        model="llama-3.1-8b-instant",  # Model adÄ±
        groq_api_key=api_key,  # API key
        temperature=0.1  # DÃ¼ÅŸÃ¼k temperature = daha tutarlÄ± yanÄ±tlar
    )

    st.info("ğŸ“š TÃ¼rk akademik tez veri seti yÃ¼kleniyor...")
    rag_documents = []  # YÃ¼klenecek belgeleri sakla

    # --- 3. DATASET YÃœKLEME ---
    try:
        # Spinner ile kullanÄ±cÄ±ya ilerleme gÃ¶ster
        with st.spinner("Dataset stream ediliyor... (Ä°lk seferde 30 saniye sÃ¼rebilir)"):
            
            # Hugging Face'ten dataset yÃ¼kle
            # streaming=True: TÃ¼m dataset'i indirmez, ihtiyaÃ§ duyulan kÄ±smÄ± alÄ±r
            dataset = load_dataset(
                "umutertugrul/turkish-academic-theses-dataset",  # Dataset adÄ±
                split="train",  # Train split'ini kullan
                streaming=True,  # Stream mode (cache gerektirmez)
                token=hf_token,  # Gated dataset iÃ§in token
                cache_dir="/tmp/huggingface"  # GeÃ§ici cache dizini
            )

            count = 0  # YÃ¼klenen belge sayacÄ±
            max_docs = 100  # Maksimum belge sayÄ±sÄ± (demo iÃ§in sÄ±nÄ±rlÄ±)
            
            # Stream'den belgeleri tek tek al
            for item in dataset:
                # Maksimum sayÄ±ya ulaÅŸtÄ±ysak dur
                if count >= max_docs:
                    break
                
                # TÃ¼rkÃ§e baÅŸlÄ±k ve Ã¶zet varsa iÅŸle
                if item.get('abstract_tr') and item.get('title_tr'):
                    # Metadata bilgilerini al
                    title = item['title_tr']  # Tez baÅŸlÄ±ÄŸÄ±
                    abstract = item['abstract_tr']  # Tez Ã¶zeti
                    author = item.get('author', 'Bilinmeyen')  # Yazar
                    year = item.get('year', 'Bilinmeyen')  # YÄ±l
                    subject = item.get('subject', 'Genel')  # Konu/alan

                    # Belge iÃ§eriÄŸini formatla
                    # Format: BaÅŸlÄ±k + Metadata + Ã–zet
                    content = f"BaÅŸlÄ±k: {title}\n\nYazar: {author}\nYÄ±l: {year}\nKonu: {subject}\n\nÃ–zet:\n{abstract}"
                    
                    # LangChain Document objesi oluÅŸtur
                    rag_documents.append(Document(
                        page_content=content,  # Ana metin
                        metadata={  # Metadata (retrieval sonrasÄ± kullanÄ±lÄ±r)
                            "source": "YÃ–K Tez Merkezi",
                            "doc_id": count,
                            "title": title,
                            "author": author,
                            "year": str(year),
                            "subject": subject
                        }
                    ))
                    count += 1  # SayacÄ± artÄ±r

        # BaÅŸarÄ± kontrolÃ¼
        if rag_documents:
            st.success(f"âœ… {len(rag_documents)} TÃ¼rk akademik tezi baÅŸarÄ±yla stream edildi!")
        else:
            # Dataset boÅŸsa exception fÄ±rlat
            raise Exception("Dataset boÅŸ dÃ¶ndÃ¼ veya eriÅŸim saÄŸlanamadÄ±.")

    except Exception as e:
        # Hata durumunda yedek veri setini kullan
        error_msg = str(e)
        st.warning(f"âš ï¸ Dataset stream edilemedi: {error_msg[:200]}...")
        st.info("ğŸ’¡ Yedek veri seti kullanÄ±lÄ±yor")

        # --- YEDEK VERÄ° SETÄ° ---
        # Dataset yÃ¼klenemezse kullanÄ±lacak Ã¶rnek tezler
        rag_documents = [
            Document(
                page_content="""BaÅŸlÄ±k: Derin Ã–ÄŸrenme YÃ¶ntemleri ile TÃ¼rkÃ§e DoÄŸal Dil Ä°ÅŸleme

Yazar: Ahmet YÄ±lmaz
YÄ±l: 2023
Konu: Bilgisayar MÃ¼hendisliÄŸi

Ã–zet:
Bu tez Ã§alÄ±ÅŸmasÄ±nda, TÃ¼rkÃ§e dil iÅŸleme gÃ¶revleri iÃ§in derin Ã¶ÄŸrenme tabanlÄ± yÃ¶ntemler geliÅŸtirilmiÅŸtir. BERT ve GPT mimarileri TÃ¼rkÃ§e korpus Ã¼zerinde fine-tune edilmiÅŸ, metin sÄ±nÄ±flandÄ±rma ve duygu analizi gÃ¶revlerinde %92.5 F1-score elde edilmiÅŸtir. TÃ¼rkÃ§eBERT modeli geliÅŸtirilmiÅŸ ve aÃ§Ä±k kaynak olarak paylaÅŸÄ±lmÄ±ÅŸtÄ±r.""",
                metadata={"source": "YÃ–K", "doc_id": 0, "title": "Derin Ã–ÄŸrenme ile TÃ¼rkÃ§e NLP", "author": "Ahmet YÄ±lmaz", "year": "2023", "subject": "Bilgisayar MÃ¼hendisliÄŸi"}
            ),
            Document(
                page_content="""BaÅŸlÄ±k: Makine Ã–ÄŸrenmesi ile TÃ¼rkiye'de Hava KirliliÄŸi Tahmini

Yazar: Zeynep Kaya
YÄ±l: 2022
Konu: Ã‡evre MÃ¼hendisliÄŸi

Ã–zet:
TÃ¼rkiye'nin bÃ¼yÃ¼k ÅŸehirlerinde hava kalitesi tahmini iÃ§in LSTM ve Random Forest algoritmalarÄ± kullanÄ±lmÄ±ÅŸtÄ±r. Ä°stanbul, Ankara ve Ä°zmir'den toplanan 5 yÄ±llÄ±k hava kalitesi verisi ile model eÄŸitilmiÅŸ, PM2.5 ve PM10 deÄŸerleri %87 doÄŸrulukla tahmin edilmiÅŸtir. Mevsimsel faktÃ¶rlerin etkisi analiz edilmiÅŸtir.""",
                metadata={"source": "YÃ–K", "doc_id": 1, "title": "ML ile Hava KirliliÄŸi Tahmini", "author": "Zeynep Kaya", "year": "2022", "subject": "Ã‡evre MÃ¼hendisliÄŸi"}
            )
        ]
        st.success(f"âœ… {len(rag_documents)} yedek TÃ¼rk akademik tezi yÃ¼klendi")

    # --- 4. METÄ°N BÃ–LME (CHUNKING) ---
    # Uzun belgeleri kÃ¼Ã§Ã¼k parÃ§alara bÃ¶l (retrieval performansÄ±nÄ± artÄ±rÄ±r)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,  # Her chunk maksimum 700 karakter
        chunk_overlap=100,  # Chunk'lar arasÄ± 100 karakter Ã¶rtÃ¼ÅŸme
        separators=["\n\n", "\n", ". ", " "]  # BÃ¶lme sÄ±rasÄ±: paragraf > satÄ±r > cÃ¼mle > kelime
    )
    chunks = text_splitter.split_documents(rag_documents)  # Belgeleri bÃ¶l
    st.info(f"ğŸ“Š {len(chunks)} metin parÃ§asÄ± oluÅŸturuldu")

    # --- 5. VEKTÃ–R VERÄ°TABANI OLUÅTUR ---
    # ChromaDB: Embedding'leri saklayan ve similarity search yapan veritabanÄ±
    vectorstore = Chroma.from_documents(
        documents=chunks,  # Chunk'lanmÄ±ÅŸ belgeler
        embedding=embedding_model,  # Embedding modeli (hash-based)
        persist_directory="/tmp/turkish_thesis_db"  # KalÄ±cÄ± depolama dizini
    )

    # --- 6. RETRIEVER OLUÅTUR ---
    # Retriever: Sorguya en benzer belgeleri bulur
    retriever = vectorstore.as_retriever(
        search_type="similarity",  # Benzerlik tabanlÄ± arama
        search_kwargs={"k": 4}  # En benzer 4 chunk'Ä± getir
    )

    # --- 7. PROMPT ÅABLONU OLUÅTUR ---
    # System prompt: Modele rolÃ¼nÃ¼ ve talimatlarÄ± ver
    system_prompt = """Sen TÃ¼rk akademik tezlerini analiz eden bir asistansÄ±n.

GÃ¶revlerin:
1. Verilen baÄŸlam (tezler) kullanarak TÃ¼rkÃ§e cevap ver
2. Tez baÅŸlÄ±klarÄ±nÄ±, yazarlarÄ±nÄ± ve yÄ±llarÄ±nÄ± belirt
3. BulgularÄ± ve yÃ¶ntemleri aÃ§Ä±kla
4. Emin olmadÄ±ÄŸÄ±n konularda "Bu bilgi verilen tezlerde yok" de

BaÄŸlam (Tezler):
{context}"""  # {context} retriever'dan gelecek

    # ChatPromptTemplate: System + User mesajlarÄ±nÄ± birleÅŸtir
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),  # System mesajÄ± (rol tanÄ±mÄ±)
        ("human", "{input}")  # User mesajÄ± (soru)
    ])

    # --- 8. RAG CHAIN'Ä° BÄ°RLEÅTÄ°R ---
    # Document chain: Belgeleri prompt'a ekle ve LLM'e gÃ¶nder
    document_chain = create_stuff_documents_chain(llm, prompt)
    
    # Retrieval chain: Retriever + Document chain'i birleÅŸtir
    # Flow: Soru -> Retriever (belgeler) -> Prompt (belgeler + soru) -> LLM -> Cevap
    rag_chain = create_retrieval_chain(retriever, document_chain)
    
    st.success("âœ… RAG Sistemi HazÄ±r!")
    return rag_chain  # KullanÄ±ma hazÄ±r RAG pipeline'Ä± dÃ¶ndÃ¼r

# =============================================================================
# STREAMLIT KULLANICI ARAYÃœZÃœ
# =============================================================================

# --- Sayfa BaÅŸlÄ±ÄŸÄ± ---
st.title("ğŸ“ Akademik Asistan")
st.markdown("**RAG + Llama 3.1** ile TÃ¼rk akademik tezlerinden bilgi Ã§Ä±karÄ±r")

# --- KullanÄ±m KÄ±lavuzu (GeniÅŸletilebilir) ---
with st.expander("â„¹ï¸ NasÄ±l KullanÄ±lÄ±r?"):
    st.markdown("""
    **Ã–rnek Sorular:**
    - "TÃ¼rkÃ§e NLP Ã§alÄ±ÅŸmalarÄ± hakkÄ±nda bilgi ver"
    - "Siber gÃ¼venlik alanÄ±nda hangi tezler var?"
    - "Makine Ã¶ÄŸrenmesi ile ilgili tezleri listele"
    - "2023 yÄ±lÄ±ndaki tezler neler?"
    - "Ã‡evre mÃ¼hendisliÄŸi alanÄ±nda neler yapÄ±lmÄ±ÅŸ?"
    """)

# =============================================================================
# API KEY ALMA FONKSÄ°YONLARI
# =============================================================================

def get_api_key():
    """
    Groq API key'i environment variable veya Streamlit secrets'tan al.
    
    Returns:
        str or None: API key veya bulunamazsa None
    """
    # Ã–nce environment variable'Ä± kontrol et
    api_key = os.environ.get("GROQ_API_KEY")
    if api_key:
        return api_key
    
    # Environment'ta yoksa Streamlit secrets'a bak
    try:
        return st.secrets["GROQ_API_KEY"]
    except:
        return None

def get_hf_token():
    """
    Hugging Face token'Ä± environment variable veya Streamlit secrets'tan al.
    Gated/private dataset'ler iÃ§in gerekli.
    
    Returns:
        str or None: HF token veya bulunamazsa None
    """
    # FarklÄ± environment variable isimlerini kontrol et
    token = (os.environ.get("HF_TOKEN") or 
             os.environ.get("HUGGING_FACE_HUB_TOKEN") or 
             os.environ.get("HUGGINGFACE_TOKEN"))
    
    if token:
        return token
    
    # Environment'ta yoksa Streamlit secrets'a bak
    try:
        return (st.secrets.get("HF_TOKEN") or 
                st.secrets.get("HUGGING_FACE_HUB_TOKEN"))
    except:
        return None

# =============================================================================
# ANA UYGULAMA AKIÅI
# =============================================================================

# API key'leri al
groq_api_key = get_api_key()
hf_token = get_hf_token()

# API key kontrolÃ¼
if groq_api_key:
    # Key bulundu, baÅŸarÄ± mesajÄ± gÃ¶ster
    st.success("âœ… Groq API Key bulundu!")
    
    # HF token kontrolÃ¼ (opsiyonel, uyarÄ± amaÃ§lÄ±)
    if not hf_token:
        st.warning("âš ï¸ Hugging Face Token (HF_TOKEN) bulunamadÄ±. Kilitli veri setine eriÅŸim baÅŸarÄ±sÄ±z olabilir.")

    # RAG sistemini kur
    rag_chain = setup_rag_system(groq_api_key, hf_token)

    # RAG sistemi baÅŸarÄ±yla kurulduysa chat arayÃ¼zÃ¼nÃ¼ gÃ¶ster
    if rag_chain:
        # --- SESSION STATE BAÅLATMA ---
        # Streamlit'te sayfa her yenilendiÄŸinde deÄŸiÅŸkenler sÄ±fÄ±rlanÄ±r.
        # session_state ile kalÄ±cÄ± veri saklayabiliriz (chat geÃ§miÅŸi gibi)
        if "messages" not in st.session_state:
            st.session_state.messages = []  # BoÅŸ mesaj listesi oluÅŸtur
        
        # --- CHAT GEÃ‡MÄ°ÅÄ°NÄ° GÃ–STER ---
        # Daha Ã¶nce yapÄ±lan konuÅŸmalarÄ± ekrana yazdÄ±r
        for message in st.session_state.messages:
            # Chat mesajÄ± bubble'Ä± oluÅŸtur (user veya assistant)
            with st.chat_message(message["role"]):
                st.markdown(message["content"])  # Mesaj iÃ§eriÄŸini gÃ¶ster
        
        # --- KULLANICI GÄ°RDÄ°SÄ° AL ---
        # Chat input box (sayfa altÄ±nda sabit kalÄ±r)
        if prompt := st.chat_input("TÃ¼rk akademik tezleri hakkÄ±nda soru sorun..."):
            # KullanÄ±cÄ± bir ÅŸey yazdÄ± ve enter'a bastÄ±
            
            # 1. KullanÄ±cÄ± mesajÄ±nÄ± session state'e ekle
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # 2. KullanÄ±cÄ± mesajÄ±nÄ± ekranda gÃ¶ster
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # --- ASISTAN CEVABINI OLUÅTUR ---
            with st.chat_message("assistant"):
                # Spinner ile iÅŸlem devam ediyor mesajÄ± gÃ¶ster
                with st.spinner("ğŸ” Tezler aranÄ±yor..."):
                    try:
                        # RAG chain'i Ã§alÄ±ÅŸtÄ±r
                        # Input: KullanÄ±cÄ± sorusu
                        # Output: {'answer': ..., 'context': [...]}
                        response = rag_chain.invoke({"input": prompt})
                        
                        # CevabÄ± al
                        answer = response['answer']
                        
                        # Kaynak belgeleri al (retriever'dan gelen chunk'lar)
                        sources = response.get('context', [])
                        
                        # EÄŸer kaynak varsa, cevabÄ±n sonuna ekle
                        if sources:
                            titles = set()  # Tekrar eden baÅŸlÄ±klarÄ± Ã¶nlemek iÃ§in set kullan
                            
                            # Her kaynak belgenin metadata'sÄ±nÄ± Ã§Ä±kar
                            for doc in sources:
                                title = doc.metadata.get('title', 'Bilinmeyen')
                                author = doc.metadata.get('author', '')
                                year = doc.metadata.get('year', '')
                                titles.add(f"{title} ({author}, {year})")  # Formatla ve ekle
                            
                            # KaynaklarÄ± cevaba ekle (maksimum 3 tane)
                            if titles:
                                answer += f"\n\nğŸ“š **Kaynak Tezler:**\n" + "\n".join([f"- {t}" for t in list(titles)[:3]])
                    
                    except Exception as e:
                        # Hata oluÅŸtuysa kullanÄ±cÄ±ya bildir
                        answer = f"âŒ Hata: {str(e)}"
                
                # CevabÄ± ekrana yazdÄ±r
                st.markdown(answer)
            
            # Asistan cevabÄ±nÄ± session state'e ekle
            st.session_state.messages.append({"role": "assistant", "content": answer})
        
        # --- TEMÄ°ZLE BUTONU ---
        # Sohbet geÃ§miÅŸini sÄ±fÄ±rla
        col1, col2 = st.columns([1, 5])  # 2 kolonlu layout
        with col1:
            if st.button("ğŸ—‘ï¸ Temizle"):
                st.session_state.messages = []  # Mesaj geÃ§miÅŸini boÅŸalt
                st.rerun()  # SayfayÄ± yenile

else:
    # API key bulunamadÄ±, hata mesajÄ± gÃ¶ster
    st.error("""
    âŒ **GROQ_API_KEY bulunamadÄ±!**
    
    **API Key almak iÃ§in:**
    1. https://console.groq.com/keys
    2. Sign up â†’ Create API Key
    3. Streamlit secrets veya environment variable olarak ekle
    """)

# =============================================================================
# SIDEBAR (YAN PANEL)
# =============================================================================

# --- Proje Bilgileri ---
st.sidebar.markdown("### ğŸ“Š Proje Bilgileri")
st.sidebar.info("""
**Veri Seti:**  
YÃ–K Tez Merkezi  
(turkish-academic-theses-dataset)

**Model:** Llama 3.1 (8B)  
**Embedding:** Hash-based  
**Vector DB:** ChromaDB  
**Framework:** LangChain
""")

# --- Ä°statistikler ---
st.sidebar.markdown("### ğŸ“Œ Ä°statistikler")
# EÄŸer mesaj varsa, kullanÄ±cÄ± soru sayÄ±sÄ±nÄ± gÃ¶ster
if 'messages' in st.session_state:
    user_questions = len([m for m in st.session_state.messages if m["role"]=="user"])
    st.sidebar.metric("Toplam Soru", user_questions)

# =============================================================================
# KOD SONU
# =============================================================================
# Bu uygulamayÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
# 1. streamlit run app.py
# 2. GROQ_API_KEY environment variable'Ä±nÄ± ayarla
# 3. (Opsiyonel) HF_TOKEN ayarla (gated dataset iÃ§in)
# =============================================================================