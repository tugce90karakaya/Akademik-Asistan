import streamlit as st
import os
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.base import Embeddings
import hashlib
from datasets import load_dataset # YENÄ° EKLENDÄ°

# --- Ã–NEMLÄ°: Hugging Face cache yollarÄ±nÄ± dÃ¼zelt ---
# Hugging Face Spaces'te Ã¶nbellek sorunlarÄ±nÄ± Ã¶nlemek iÃ§in geÃ§ici dizin kullanÄ±r
os.environ["HF_HOME"] = "/tmp/huggingface"
os.environ["HF_DATASETS_CACHE"] = "/tmp/huggingface/datasets"
os.environ["TRANSFORMERS_CACHE"] = "/tmp/huggingface/transformers"
os.environ["XDG_CACHE_HOME"] = "/tmp/huggingface"
os.environ["HF_HUB_CACHE"] = "/tmp/huggingface/hub"

os.makedirs("/tmp/huggingface/datasets", exist_ok=True)
os.makedirs("/tmp/huggingface/transformers", exist_ok=True)
os.makedirs("/tmp/huggingface/hub", exist_ok=True)

# --- Streamlit yapÄ±landÄ±rmasÄ± ---
st.set_page_config(
    page_title="TÃ¼rk Akademik Tez AsistanÄ±",
    page_icon="ğŸ“",
    layout="wide"
)

# --- Hash tabanlÄ± embedding sÄ±nÄ±fÄ± ---
class SimpleHashEmbeddings(Embeddings):
    """Hash tabanlÄ± embedding - model indirmeye gerek yok"""
    def __init__(self, dimension=384):
        self.dimension = dimension
    
    def embed_documents(self, texts):
        embeddings = []
        for text in texts:
            hash_obj = hashlib.sha256(text.encode())
            hash_bytes = hash_obj.digest()
            vector = [float(b) / 255.0 for b in hash_bytes[:self.dimension]]
            vector += [0.0] * (self.dimension - len(vector))
            embeddings.append(vector)
        return embeddings
    
    def embed_query(self, text):
        return self.embed_documents([text])[0]

# --- RAG sistemi kurulumu ---
@st.cache_resource
def setup_rag_system(api_key, hf_token=None):
    if not api_key:
        st.error("âŒ GROQ_API_KEY bulunamadÄ±.")
        return None
    
    st.info("ğŸš€ RAG sistemi baÅŸlatÄ±lÄ±yor...")

    embedding_model = SimpleHashEmbeddings(dimension=256)
    llm = ChatGroq(
        model="llama-3.1-8b-instant",
        groq_api_key=api_key,
        temperature=0.1
    )

    st.info("ğŸ“š TÃ¼rk akademik tez veri seti yÃ¼kleniyor...")
    rag_documents = []

    # --- Dataset yÃ¼kleme ---
    try:
        
        with st.spinner("Dataset stream ediliyor... (Ä°lk seferde 30 saniye sÃ¼rebilir)"):
            # load_dataset kullanÄ±mÄ±, hf_token ile yetkilendirme saÄŸlar
            dataset = load_dataset(
                "umutertugrul/turkish-academic-theses-dataset",
                split="train",
                streaming=True, 
                token=hf_token, # HF Token'Ä± burada kullanÄ±yoruz
                cache_dir="/tmp/huggingface"
            )

            count = 0
            max_docs = 100 # Demo iÃ§in 100 tezle sÄ±nÄ±rlÄ±yoruz
            for item in dataset:
                if count >= max_docs:
                    break
                if item.get('abstract_tr') and item.get('title_tr'):
                    title = item['title_tr']
                    abstract = item['abstract_tr']
                    author = item.get('author', 'Bilinmeyen')
                    year = item.get('year', 'Bilinmeyen')
                    subject = item.get('subject', 'Genel')

                    content = f"BaÅŸlÄ±k: {title}\n\nYazar: {author}\nYÄ±l: {year}\nKonu: {subject}\n\nÃ–zet:\n{abstract}"
                    rag_documents.append(Document(
                        page_content=content,
                        metadata={
                            "source": "YÃ–K Tez Merkezi",
                            "doc_id": count,
                            "title": title,
                            "author": author,
                            "year": str(year),
                            "subject": subject
                        }
                    ))
                    count += 1

        if rag_documents:
            st.success(f"âœ… {len(rag_documents)} TÃ¼rk akademik tezi baÅŸarÄ±yla stream edildi!")
        else:
            raise Exception("Dataset boÅŸ dÃ¶ndÃ¼ veya eriÅŸim saÄŸlanamadÄ±.")

    except Exception as e:
        error_msg = str(e)
        st.warning(f"âš ï¸ Dataset stream edilemedi: {error_msg[:200]}...")
        st.info("ğŸ’¡ Yedek veri seti kullanÄ±lÄ±yor")

        # --- Yedek veri seti ---
        rag_documents = [
            Document(
                page_content="""BaÅŸlÄ±k: Derin Ã–ÄŸrenme YÃ¶ntemleri ile TÃ¼rkÃ§e DoÄŸal Dil Ä°ÅŸleme

Yazar: Ahmet YÄ±lmaz
YÄ±l: 2023
Konu: Bilgisayar MÃ¼hendisliÄŸi

Ã–zet:
Bu tez Ã§alÄ±ÅŸmasÄ±nda, TÃ¼rkÃ§e dil iÅŸleme gÃ¶revleri iÃ§in derin Ã¶ÄŸrenme tabanlÄ± yÃ¶ntemler geliÅŸtirilmiÅŸtir. BERT ve GPT mimarileri TÃ¼rkÃ§e korpus Ã¼zerinde fine-tune edilmiÅŸ, metin sÄ±nÄ±flandÄ±rma ve duygu analizi gÃ¶revlerinde %92.5 F1-score elde edilmiÅŸtir. TÃ¼rkÃ§eBERT modeli geliÅŸtirilmiÅŸ ve aÃ§Ä±k kaynak olarak paylaÅŸÄ±lmÄ±ÅŸtÄ±r.""",
                metadata={"source": "YÃ–K", "doc_id": 0, "title": "Derin Ã–ÄŸrenme ile TÃ¼rkÃ§e NLP", "author": "Ahmet YÄ±lmaz", "year": "2023", "subject": "Bilgisayar MÃ¼hendisliÄŸi"}
            ),
            Document(
                page_content="""BaÅŸlÄ±k: Makine Ã–ÄŸrenmesi ile TÃ¼rkiye'de Hava KirliliÄŸi Tahmini

Yazar: Zeynep Kaya
YÄ±l: 2022
Konu: Ã‡evre MÃ¼hendisliÄŸi

Ã–zet:
TÃ¼rkiye'nin bÃ¼yÃ¼k ÅŸehirlerinde hava kalitesi tahmini iÃ§in LSTM ve Random Forest algoritmalarÄ± kullanÄ±lmÄ±ÅŸtÄ±r. Ä°stanbul, Ankara ve Ä°zmir'den toplanan 5 yÄ±llÄ±k hava kalitesi verisi ile model eÄŸitilmiÅŸ, PM2.5 ve PM10 deÄŸerleri %87 doÄŸrulukla tahmin edilmiÅŸtir. Mevsimsel faktÃ¶rlerin etkisi analiz edilmiÅŸtir.""",
                metadata={"source": "YÃ–K", "doc_id": 1, "title": "ML ile Hava KirliliÄŸi Tahmini", "author": "Zeynep Kaya", "year": "2022", "subject": "Ã‡evre MÃ¼hendisliÄŸi"}
            )
        ]
        st.success(f"âœ… {len(rag_documents)} yedek TÃ¼rk akademik tezi yÃ¼klendi")

    # --- Text splitting ---
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,
        chunk_overlap=100,
        separators=["\n\n", "\n", ". ", " "]
    )
    chunks = text_splitter.split_documents(rag_documents)
    st.info(f"ğŸ“Š {len(chunks)} metin parÃ§asÄ± oluÅŸturuldu")

    # --- VektÃ¶r veritabanÄ± ---
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        persist_directory="/tmp/turkish_thesis_db"
    )

    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}
    )

    # --- TÃ¼rkÃ§e prompt ---
    system_prompt = """Sen TÃ¼rk akademik tezlerini analiz eden bir asistansÄ±n.

GÃ¶revlerin:
1. Verilen baÄŸlam (tezler) kullanarak TÃ¼rkÃ§e cevap ver
2. Tez baÅŸlÄ±klarÄ±nÄ±, yazarlarÄ±nÄ± ve yÄ±llarÄ±nÄ± belirt
3. BulgularÄ± ve yÃ¶ntemleri aÃ§Ä±kla
4. Emin olmadÄ±ÄŸÄ±n konularda "Bu bilgi verilen tezlerde yok" de

BaÄŸlam (Tezler):
{context}"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])

    document_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, document_chain)
    
    st.success("âœ… RAG Sistemi HazÄ±r!")
    return rag_chain

# --- UI ---
st.title("ğŸ“ TÃ¼rk Akademik Tez AraÅŸtÄ±rma AsistanÄ±")
st.markdown("**RAG + Llama 3.1** ile TÃ¼rk akademik tezlerinden bilgi Ã§Ä±karÄ±r")

with st.expander("â„¹ï¸ NasÄ±l KullanÄ±lÄ±r?"):
    st.markdown("""
    **Ã–rnek Sorular:**
    - "TÃ¼rkÃ§e NLP Ã§alÄ±ÅŸmalarÄ± hakkÄ±nda bilgi ver"
    - "Siber gÃ¼venlik alanÄ±nda hangi tezler var?"
    - "Makine Ã¶ÄŸrenmesi ile ilgili tezleri listele"
    - "2023 yÄ±lÄ±ndaki tezler neler?"
    - "Ã‡evre mÃ¼hendisliÄŸi alanÄ±nda neler yapÄ±lmÄ±ÅŸ?"
    """)

# --- API key fonksiyonlarÄ± ---
def get_api_key():
    # Groq API Key
    api_key = os.environ.get("GROQ_API_KEY")
    if api_key:
        return api_key
    try:
        return st.secrets["GROQ_API_KEY"]
    except:
        return None

def get_hf_token():
    # Hugging Face Token (Gated Dataset EriÅŸimi Ä°Ã§in)
    # st.secrets, HF_TOKEN veya HUGGING_FACE_HUB_TOKEN olarak ayarlanmÄ±ÅŸ olabilir.
    token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
    if token:
        return token
    try:
        return st.secrets.get("HF_TOKEN") or st.secrets.get("HUGGING_FACE_HUB_TOKEN")
    except:
        return None

groq_api_key = get_api_key()
hf_token = get_hf_token()

# --- Ana RAG Ã§alÄ±ÅŸma akÄ±ÅŸÄ± ---
if groq_api_key:
    st.success("âœ… Groq API Key bulundu!")
    
    # HF Token'Ä±n varlÄ±ÄŸÄ±nÄ± kontrol ediyoruz
    if not hf_token:
        st.warning("âš ï¸ Hugging Face Token (HF_TOKEN) bulunamadÄ±. Kilitli veri setine eriÅŸim baÅŸarÄ±sÄ±z olabilir.")

    rag_chain = setup_rag_system(groq_api_key, hf_token)

    if rag_chain:
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        if prompt := st.chat_input("TÃ¼rk akademik tezleri hakkÄ±nda soru sorun..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            with st.chat_message("assistant"):
                with st.spinner("ğŸ” Tezler aranÄ±yor..."):
                    try:
                        response = rag_chain.invoke({"input": prompt})
                        answer = response['answer']
                        sources = response.get('context', [])
                        if sources:
                            titles = set()
                            for doc in sources:
                                title = doc.metadata.get('title', 'Bilinmeyen')
                                author = doc.metadata.get('author', '')
                                year = doc.metadata.get('year', '')
                                titles.add(f"{title} ({author}, {year})")
                            if titles:
                                answer += f"\n\nğŸ“š **Kaynak Tezler:**\n" + "\n".join([f"- {t}" for t in list(titles)[:3]])
                    except Exception as e:
                        answer = f"âŒ Hata: {str(e)}"
                st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        
        col1, col2 = st.columns([1, 5])
        with col1:
            if st.button("ğŸ—‘ï¸ Temizle"):
                st.session_state.messages = []
                st.rerun()

else:
    st.error("""
    âŒ **GROQ_API_KEY bulunamadÄ±!**
    
    **API Key almak iÃ§in:**
    1. https://console.groq.com/keys
    2. Sign up â†’ Create API Key
    3. Streamlit secrets veya environment variable olarak ekle
    """)

# --- Sidebar ---
st.sidebar.markdown("### ğŸ“Š Proje Bilgileri")
st.sidebar.info("""
**Veri Seti:** Â 
YÃ–K Tez Merkezi Â 
(turkish-academic-theses-dataset)

**Model:** Llama 3.1 (8B) Â 
**Embedding:** Hash-based Â 
**Vector DB:** ChromaDB Â 
**Framework:** LangChain
""")

st.sidebar.markdown("### ğŸ“Œ Ä°statistikler")
if 'messages' in st.session_state:
    st.sidebar.metric("Toplam Soru", len([m for m in st.session_state.messages if m["role"]=="user"]))